{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/aprendizaje-automatico-dc-uba-ar/material/blob/main/notebooks/notebook_05_seleccion_modelos-published.ipynb)\n", "\n", "# Selecci\u00f3n de modelos"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["%matplotlib inline\n", "import numpy as np\n", "import matplotlib.pyplot as plt\n", "import pandas as pd\n", "from IPython.display import display\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Cross validation \n", "\n", "Hasta ahora s\u00f3lo hab\u00edamos visto (ver en el [notebook 03](https://github.com/aprendizaje-automatico-dc-uba-ar/material/blob/main/notebooks/notebook_03_arboles_de_decision_sklearn-published.ipynb)) que ibamos a dividir los datos en train y test.\n", "\n", "\n", "En esta semana vimos la opci\u00f3n de hacer validaci\u00f3n cruzada. En esta oportunidad lo que haremos sera realizar una exploraci\u00f3n de hiperpar\u00e1metros para para \u00e1rboles incorporando conceptos de la clase de esta semana.\n", "Vamos a experimentar usando k-fold (con k=10) para explorar distintos valores de configuraci\u00f3n de `DecisionTreeClassifier` para seleccionar el hiperpar\u00e1metro que nos parezca el mejor. \n", "Ensayaremos \u00e1ltura m\u00e1xima con valores `[None, 1, 2, 3, 5, 8, 13, 21]`. \n", "\n", "Nos interesar\u00e1:\n", "- controlar el tiempo de entrenamiento\n", "- generar alguna m\u00e9trica que elijamos para seleccionar la \u00e1ltura m\u00e1xima\n", "\n", "Con la mejor configuraci\u00f3n obtenida entrenar un clasificador con todos los datos de desarrollo.\n", "    \n", "Evaluar el comportamiento con el set de evaluaci\u00f3n\n", "    \n", "\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import numpy as np\n", "from sklearn.model_selection import train_test_split, KFold\n", "from sklearn.tree import DecisionTreeClassifier\n", "import timeit\n", "import pandas as pd\n", "\n", "def cargar_datos():\n", "    df = pd.read_csv('https://github.com/aprendizaje-automatico-dc-uba-ar/material/raw/main/dataset/data_05/seleccion_modelos.csv')\n", "    X = df.drop(\"Y\", axis=1)\n", "    y = df.Y\n", "    return X.to_numpy(), y.to_numpy()\n", "\n", "X, y = cargar_datos()\n", "X,y"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Primero separaremos nuestro data set entre **desarrollo** y **evaluaci\u00f3n** en un 10%. Para esto podemos usar `train_test_split`"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# separamos entre dev y eval\n", "X_dev, X_eval, y_dev, y_eval = train_test_split(\n", "                    X, \n", "                    y, \n", "                    random_state=4, \n", "                    test_size=0.1)\n", "type(y_eval)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Por el momento dejaremos el set de evaluaci\u00f3n de lado y nos manejaremos con el de desarrollo.\n", "\n", "Pasemos a experimentar los distinos `h_max` posibles.\n", "\n", "Usaremos estas dos funciones para entrenar un \u00e1rbol y para usarlo para predecir respectivamente:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def train_tree(X_tr: np.ndarray, y_tr: np.ndarray, tree_params={}) -> DecisionTreeClassifier:\n", "    arbol = DecisionTreeClassifier(**tree_params)\n", "    arbol.fit(X_tr, y_tr)\n", "\n", "    return arbol\n", "\n", "def tree_predict(ab: DecisionTreeClassifier, X_test: np.ndarray) -> np.ndarray:\n", "    predictions = ab.predict(X_test)\n", "    return predictions"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Y definimos la m\u00e9trica a usar. A modo de ejemplo figura accuracy.\n", "\n", "Cambiar la medida por una nueva vista en clase."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def accuracy(y_predicted: np.ndarray, y_real: np.ndarray) -> float:\n", "    TP_TN = sum([y_i == y_j for (y_i, y_j) in zip(y_predicted, y_real)]) \n", "    P_N = len(y_real)\n", "    return TP_TN /P_N\n", "\n", "def metrica_seleccionada(y_predicted: np.ndarray, y_real: np.ndarray) -> float:\n", "    return accuracy(y_predicted, y_real)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Realizaci\u00f3n del experimento.\n", "\n", "Nota: se inicializa con una semilla para poder reproducir el resultado."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["results = []\n", "\n", "np.random.seed(44)\n", "for h_max in [None, 1, 2, 3, 5, 8, 13, 21]:\n", "    kf = KFold(n_splits=10)    \n", "    y_pred = np.empty(y_dev.shape)\n", "    y_pred.fill(np.nan)\n", "    \n", "    # generamos para cada fold una predicci\u00f3n\n", "    for train_index, test_index in kf.split(X_dev):\n", "        \n", "        #saco el fold que no uso para entrenar\n", "        kf_X_train, kf_X_test = X_dev[train_index], X_dev[test_index]\n", "        kf_y_train, kf_y_test = y_dev[train_index], y_dev[test_index]\n", "\n", "        current_tree = train_tree(kf_X_train, kf_y_train,\n", "                                    tree_params={\"max_depth\":h_max})\n", "        predictions = tree_predict(current_tree, kf_X_test)\n", "        y_pred[test_index] = predictions\n", "        \n", "    current_score = metrica_seleccionada(y_pred, y_dev)\n", "        \n", "    results.append((h_max,current_score))\n", "    \n", "\n", "# Ordenamos los resultados (puede ser que convenga del derecho o del reves)\n", "r = sorted(results, key=lambda x: x[1], reverse=True)\n", "\n", "print(\"\u00d3rden obtenido seg\u00fan la m\u00e9trica elegida\")\n", "for idx, (h, sc) in enumerate(r):\n", "    print(f\"\\t{idx+1}- h_max={h} con {sc:.3f}\")\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Con los resultados obtenidos podemos elegir la `h_max` que nos parezca mejor. Con eso vamos a reentrenar el modelo con todos los datos. \n", "\n", "\u00bfQu\u00e9 ten\u00edamos que tener en cuenta en estos casos?"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# elegimos\n", "h_max = None # COMPLETAR\n", "selection_score = None # COMPLETAR\n"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["assert selection_score is not None, \"Completar la celda anterior para continuar\""]}, {"cell_type": "markdown", "metadata": {}, "source": ["Con el mejor par\u00e1metro entrenamos un nuevo clasificador:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["print(f\"Construimos nuestro clasificador con par\u00e1metro 'max_depth'={h_max}.\"\n", "     + f\"Para seleccionarlo el score que hab\u00edamos obtenido era {selection_score:.3f}\")\n", "\n", "best_tree = train_tree(X_dev, y_dev,\n", "                            tree_params={\"max_depth\":h_max})\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Podemos evaluar este \u00e1rbol en train."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["y_pred = tree_predict(best_tree, X_dev)       \n", "best_tree_score = metrica_seleccionada(y_pred, y_dev)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["\u00bfQu\u00e9 nos est\u00e1n diciendo estos dos scores?\u00bfPara qu\u00e9 nos sirven?"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Por \u00fanica vez vemos como funciona nuestro entrenamiento en los datos de **evaluaci\u00f3n** que no hab\u00edamos mirado."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["y_pred_eval = tree_predict(best_tree, X_eval)       \n", "best_tree_score_eval = metrica_seleccionada(y_pred_eval, y_eval)\n", "\n", "print(f\"Con el \u00e1rbol entrenado con el par\u00e1metro seleccionado tenemos en eval un score de {best_tree_score_eval:.3f}\")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Opcionales\n", "\n", "1. Simular qu\u00e9 hubiese ocurrido si hubieramos elegido un K distinto. \u00bfLa diferencia entre el score en *dev* y el score en *eval* cambia significativamente?\n", "2. Repetir el mismo ejercicio de elegir la mejor combinaci\u00f3n de parametros pero esta vez establecer una grilla donde se exploren al menos dos hiperpar\u00e1metros que no sean la altura m\u00e1xima. Revisar la documentaci\u00f3n de `DecisionTreeClassifier`, por ejemplo pueden elegir la **medida de impureza** y el **m\u00ednimo de muestas necesario para realizar un split**. Definir los rangos necesarios para explorar m\u00e1s de un valor de cada hiperpar\u00e1metro considerado. \u00bfEste modelo fue mejor que el obtenido en el punto anterior?\n", "\n", "**Importante**: en este punto nos tomamos la licencia de usar nuevamente el conjunto de evaluaci\u00f3n. El re-uso de el conjunto de evaluaci\u00f3n s\u00f3lo lo permitimos en este caso por motivos pedag\u00f3cios. Pero **NO DEBE** suceder en la pr\u00e1ctica.\n", "\n"]}], "metadata": {"language_info": {"name": "python"}}, "nbformat": 4, "nbformat_minor": 2}