{"cells":[{"cell_type":"markdown","metadata":{},"source":["[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/aprendizaje-automatico-dc-uba-ar/material/blob/main/notebooks/notebook_05_seleccion_modelos-published.ipynb)\n","\n","# Selección de modelos"]},{"cell_type":"code","execution_count":33,"metadata":{},"outputs":[],"source":["%matplotlib inline\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import pandas as pd\n","from IPython.display import display\n"]},{"cell_type":"markdown","metadata":{},"source":["## Cross validation \n","\n","Hasta ahora sólo habíamos visto (ver en el [notebook 03](https://github.com/aprendizaje-automatico-dc-uba-ar/material/blob/main/notebooks/notebook_03_arboles_de_decision_sklearn-published.ipynb)) que ibamos a dividir los datos en train y test.\n","\n","\n","En esta semana vimos la opción de hacer validación cruzada. En esta oportunidad lo que haremos sera realizar una exploración de hiperparámetros para árboles incorporando conceptos de la clase de esta semana.\n","Vamos a experimentar usando k-fold (con k=10) para explorar distintos valores de configuración de `DecisionTreeClassifier` para seleccionar el hiperparámetro que nos parezca el mejor. \n","Ensayaremos áltura máxima con valores `[None, 1, 2, 3, 5, 8, 13, 21]`. \n","\n","Nos interesará:\n","- controlar el tiempo de entrenamiento\n","- generar alguna métrica que elijamos para seleccionar la áltura máxima\n","\n","Con la mejor configuración obtenida entrenar un clasificador con todos los datos de desarrollo.\n","    \n","Evaluar el comportamiento con el set de evaluación\n","    \n","\n"]},{"cell_type":"code","execution_count":28,"metadata":{},"outputs":[{"data":{"text/plain":["(array([[ 0.74946762, -1.83875845,  2.31697643, ...,  0.38502105,\n","          1.15910799,  0.36490854],\n","        [ 1.36142303,  0.17739336, -1.06308644, ..., -0.00426734,\n","         -1.63632588, -0.8335227 ],\n","        [ 0.12238178,  1.03817562, -1.46411856, ...,  1.69000604,\n","         -0.57898546,  0.34605186],\n","        ...,\n","        [ 0.77302083,  0.76832206, -0.36434009, ..., -0.05485574,\n","         -0.51528272,  0.7993889 ],\n","        [-0.54238642, -0.87839139,  0.68624112, ...,  0.20799802,\n","          1.06110671, -0.34658297],\n","        [-0.03135099,  0.93928815, -1.16413366, ...,  0.73422269,\n","         -0.37504853, -0.59041732]]),\n"," array([1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0,\n","        1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0,\n","        1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n","        0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0,\n","        0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0,\n","        1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n","        1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0], dtype=int64))"]},"execution_count":28,"metadata":{},"output_type":"execute_result"}],"source":["import numpy as np\n","from sklearn.model_selection import train_test_split, KFold\n","from sklearn.tree import DecisionTreeClassifier\n","import timeit\n","import pandas as pd\n","\n","def cargar_datos():\n","    df = pd.read_csv('https://github.com/aprendizaje-automatico-dc-uba-ar/material/raw/main/dataset/data_05/seleccion_modelos.csv')\n","    X = df.drop(\"Y\", axis=1) #saca la columna del target\n","    y = df.Y\n","    return X.to_numpy(), y.to_numpy() #paso el dataset a formato array\n","\n","X, y = cargar_datos()\n","X,y"]},{"cell_type":"markdown","metadata":{},"source":["Primero separaremos nuestro data set entre **desarrollo** y **evaluación** en un 10%. Para esto podemos usar `train_test_split`"]},{"cell_type":"code","execution_count":29,"metadata":{},"outputs":[{"data":{"text/plain":["numpy.ndarray"]},"execution_count":29,"metadata":{},"output_type":"execute_result"}],"source":["# separamos entre dev y eval\n","X_dev, X_eval, y_dev, y_eval = train_test_split(\n","                    X, \n","                    y, \n","                    random_state=4, #una semilla para mezclarlo siempre igual\n","                    test_size=0.1) #10% de evaluacion\n","type(y_eval)"]},{"cell_type":"code","execution_count":30,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Longitud original: 150\n","Longitud del set de desarrollo: 135\n","Longitud del set de evaluación: 15\n"]}],"source":["print(\"Longitud original:\", len(y))\n","print(\"Longitud del set de desarrollo:\", len(y_dev))\n","print(\"Longitud del set de evaluación:\", len(y_eval))"]},{"cell_type":"markdown","metadata":{},"source":["Por el momento dejaremos el set de evaluación de lado y nos manejaremos con el de desarrollo.\n","\n","Pasemos a experimentar los distintos `h_max` posibles.\n","\n","Usaremos estas dos funciones para entrenar un árbol y para usarlo para predecir respectivamente:"]},{"cell_type":"code","execution_count":31,"metadata":{},"outputs":[],"source":["def train_tree(X_tr: np.ndarray, y_tr: np.ndarray, tree_params={}) -> DecisionTreeClassifier:\n","    arbol = DecisionTreeClassifier(**tree_params) #crea el arbol con ciertos hiperparametros que le pasas: la altura maxima \n","    arbol.fit(X_tr, y_tr)\n","\n","    return arbol\n","\n","def tree_predict(ab: DecisionTreeClassifier, X_test: np.ndarray) -> np.ndarray:\n","    predictions = ab.predict(X_test) #le pasas el arbol ya entrenado y te devuelve las predicciones del test\n","    return predictions"]},{"cell_type":"markdown","metadata":{},"source":["Y definimos la métrica a usar. A modo de ejemplo figura accuracy.\n","\n","Cambiar la medida por una nueva vista en clase."]},{"cell_type":"code","execution_count":61,"metadata":{},"outputs":[],"source":["def accuracy(y_predicted: np.ndarray, y_real: np.ndarray) -> float:\n","    TP_TN = sum([y_i == y_j for (y_i, y_j) in zip(y_predicted, y_real)]) \n","    P_N = len(y_real)\n","    return TP_TN /P_N\n","\n","def precision_recall(y_predicted: np.ndarray, y_real: np.ndarray) -> np.ndarray:\n","    TP = sum([y_i == 1 and y_j == 1 for (y_i, y_j) in zip(y_predicted, y_real)])\n","    FP = sum([y_i == 1 and y_j == 0 for (y_i, y_j) in zip(y_predicted, y_real)])\n","    FN = sum([y_i == 0 and y_j == 1 for (y_i, y_j) in zip(y_predicted, y_real)])\n","    precision = TP / (TP + FP)\n","    recall = TP / (TP + FN)\n","    return precision, recall\n","\n","def f_score(y_predicted: np.ndarray, y_real: np.ndarray, beta = 0.5) -> float:\n","    presicion, recall = precision_recall(y_predicted, y_real)\n","    f_score = (1 + beta**2) * (presicion * recall) / ((beta**2) * presicion + recall)\n","    return f_score\n","\n","def metrica_seleccionada(y_predicted: np.ndarray, y_real: np.ndarray) -> float:\n","    return accuracy(y_predicted, y_real)"]},{"cell_type":"code","execution_count":36,"metadata":{},"outputs":[{"data":{"text/plain":["(1.0, 0.6666666666666666)"]},"execution_count":36,"metadata":{},"output_type":"execute_result"}],"source":["metrica_seleccionada(np.array([0,1,0,1]), np.array([1,1,0,1])) #veamos un ejemplito a ver si funciona bien"]},{"cell_type":"markdown","metadata":{},"source":["Realización del experimento.\n","\n","Nota: se inicializa con una semilla para poder reproducir el resultado."]},{"cell_type":"code","execution_count":68,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Órden obtenido según la métrica elegida\n","\t1- h_max=1 con 0.881\n","\t2- h_max=3 con 0.874\n","\t3- h_max=2 con 0.867\n","\t4- h_max=5 con 0.859\n","\t5- h_max=8 con 0.830\n","\t6- h_max=None con 0.822\n","\t7- h_max=13 con 0.800\n","\t8- h_max=21 con 0.800\n"]}],"source":["results = []\n","\n","np.random.seed(44)\n","for h_max in [None, 1, 2, 3, 5, 8, 13, 21]:\n","    kf = KFold(n_splits=10) #voy a usar 10 folds\n","    y_pred = np.empty(y_dev.shape)\n","    y_pred.fill(np.nan)\n","    \n","    # generamos para cada fold una predicción\n","    for train_index, test_index in kf.split(X_dev): \n","        #en train_index estan los indices de los datos que estan en los 9 folds que pertenecen a training\n","        #en test_index estan los indices de los datos que estan en el unico fold que es el test\n","        \n","        #saco el fold que no uso para entrenar\n","        kf_X_train, kf_X_test = X_dev[train_index], X_dev[test_index]\n","        kf_y_train, kf_y_test = y_dev[train_index], y_dev[test_index]\n","\n","        current_tree = train_tree(kf_X_train, kf_y_train,\n","                                    tree_params={\"max_depth\":h_max})\n","        predictions = tree_predict(current_tree, kf_X_test)\n","        y_pred[test_index] = predictions #quedan algunos vacios (con NAs) en cada iteracion pero finalmente se llena todo\n","        \n","    current_score = metrica_seleccionada(y_pred, y_dev) #mido que tan bien me fue con las predicciones\n","    \n","    results.append((h_max,current_score)) #para cada altura se guarda la performance\n","    \n","\n","# Ordenamos los resultados (puede ser que convenga del derecho o del reves) por score de mayor a menor\n","r = sorted(results, key=lambda x: x[1], reverse=True)\n","\n","print(\"Órden obtenido según la métrica elegida\")\n","for idx, (h, sc) in enumerate(r):\n","    print(f\"\\t{idx+1}- h_max={h} con {sc:.3f}\")\n"]},{"cell_type":"markdown","metadata":{},"source":["Con los resultados obtenidos podemos elegir la `h_max` que nos parezca mejor. Con eso vamos a reentrenar el modelo con todos los datos. \n","\n","¿Qué teníamos que tener en cuenta en estos casos?"]},{"cell_type":"code","execution_count":69,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["1\n","0.8814814814814815\n"]}],"source":["# elegimos la altura 1 porque es el que más F-score tuvo\n","h_max = r[0][0] \n","selection_score = r[0][1] \n","print(h_max)\n","print(selection_score)"]},{"cell_type":"code","execution_count":64,"metadata":{},"outputs":[],"source":["assert selection_score is not None, \"Completar la celda anterior para continuar\""]},{"cell_type":"markdown","metadata":{},"source":["Con el mejor parámetro entrenamos un nuevo clasificador:"]},{"cell_type":"code","execution_count":65,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Construimos nuestro clasificador con parámetro 'max_depth'=1.\n","Para seleccionarlo el score que habíamos obtenido era 0.881\n"]}],"source":["print(f\"Construimos nuestro clasificador con parámetro 'max_depth'={h_max}.\"\n","     + f\"\\nPara seleccionarlo el score que habíamos obtenido era {selection_score:.3f}\")\n","\n","best_tree = train_tree(X_dev, y_dev,\n","                            tree_params={\"max_depth\":h_max})\n"]},{"cell_type":"markdown","metadata":{},"source":["Podemos evaluar este árbol en train."]},{"cell_type":"code","execution_count":66,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["El F-score del árbol seleccionado es 0.904\n"]}],"source":["y_pred = tree_predict(best_tree, X_dev)\n","best_tree_score = metrica_seleccionada(y_pred, y_dev)\n","print(f\"El F-score del árbol seleccionado es {best_tree_score:.3f}\")"]},{"cell_type":"markdown","metadata":{},"source":["¿Qué nos están diciendo estos dos scores?¿Para qué nos sirven?"]},{"cell_type":"markdown","metadata":{},"source":["Primer F-score: es el obtenido con el método de 10-folds cross validation aplicado a cada altura máxima posible (de la lista de alturas dada). Con este seleccionamos la mejor altura: h_max.\n","\n","Segundo F-score: es el obtenido sin el método de cross validation (entreno y testeo con los mismos datos pero con altura máxima = 1 que es la que me dió con el método de 10-folds cross validation).\n","\n","¿Para qué nos sirven? \n","El primero sirve para elegir el h_max, y el segundo, ya teniendo el h_max, vemos que tan bien predice los datos habiendo entrenado con esos mismos."]},{"cell_type":"markdown","metadata":{},"source":["Por única vez vemos como funciona nuestro entrenamiento en los datos de **evaluación** que no habíamos mirado."]},{"cell_type":"code","execution_count":70,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Con el árbol entrenado con el parámetro seleccionado tenemos en eval un score de 0.733\n"]}],"source":["y_pred_eval = tree_predict(best_tree, X_eval)       \n","best_tree_score_eval = metrica_seleccionada(y_pred_eval, y_eval)\n","\n","print(f\"Con el árbol entrenado con el parámetro seleccionado tenemos en eval un score de {best_tree_score_eval:.3f}\")"]},{"cell_type":"markdown","metadata":{},"source":["Nos dió re mal. Muy probablemente es porque la métrica que elegimos es malisima"]},{"cell_type":"markdown","metadata":{},"source":["## Opcionales\n","\n","1. Simular qué hubiese ocurrido si hubieramos elegido un K distinto. ¿La diferencia entre el score en *dev* y el score en *eval* cambia significativamente?\n","2. Repetir el mismo ejercicio de elegir la mejor combinación de parametros pero esta vez establecer una grilla donde se exploren al menos dos hiperparámetros que no sean la altura máxima. Revisar la documentación de `DecisionTreeClassifier`, por ejemplo pueden elegir la **medida de impureza** y el **mínimo de muestas necesario para realizar un split**. Definir los rangos necesarios para explorar más de un valor de cada hiperparámetro considerado. ¿Este modelo fue mejor que el obtenido en el punto anterior?\n","\n","**Importante**: en este punto nos tomamos la licencia de usar nuevamente el conjunto de evaluación. El re-uso de el conjunto de evaluación sólo lo permitimos en este caso por motivos pedagócios. Pero **NO DEBE** suceder en la práctica.\n","\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.2"}},"nbformat":4,"nbformat_minor":2}
